{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chungichien/Desktop/資產配置/trade/quant_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from get_data import IB_data, YF_data, AV_data\n",
    "from pytorch_fit import set_seed, setup_dataloader, Test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import talib\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as precision_recall_fscore\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取資料(IB, yfinance or alpha_vantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_IB = IB_data('EUR', 'USD', endDateTime='20220726 23:59:59')\n",
    "# df_YF = YF_data('EUR', 'USD', start='2012-07-30', end='2022-07-26')\n",
    "df_AV = AV_data('EUR', 'USD', start='2012-07-30', end='2022-07-26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算指標(頻率為小時)及製作模型輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取出匯率每小時收盤價\n",
    "# data = df[['Close']]\n",
    "\n",
    "# # 計算匯率日報酬\n",
    "# day_data = data.resample('D').last().dropna()\n",
    "# day_rtn = day_data.pct_change().dropna()\n",
    "# day_rtn\n",
    "\n",
    "# hourly_std_byday = data.pct_change().resample('D').std().dropna()\n",
    "# hourly_skew_byday = 3 * (data.pct_change().resample('D').mean().dropna() - data.pct_change().resample('D').median().dropna()) \\\n",
    "#                        / data.pct_change().resample('D').std().dropna()\n",
    "# cumu_24day_return = ((day_rtn + 1).rolling(window=24).apply(np.prod, raw=True) - 1).dropna()\n",
    "\n",
    "# data = df\n",
    "# data = data.drop(index=data.index[-1])\n",
    "# # print(data.info())\n",
    "\n",
    "# rtn = data.pct_change().dropna()\n",
    "# # print(rtn.info())\n",
    "\n",
    "# mean_last24hour = rtn.rolling(24).mean().dropna()\n",
    "# # print(mean_last24hour.info())\n",
    "\n",
    "# std_last24hour = rtn.rolling(24).std().dropna()\n",
    "# # print(std_last24hour.info())\n",
    "\n",
    "# median_last24hour = rtn.rolling(24).median().dropna()\n",
    "# # print(median_last24hour.info())\n",
    "\n",
    "# skew_last24hour = 3 * (mean_last24hour - median_last24hour) / std_last24hour\n",
    "# # print(skew_last24hour.info())\n",
    "\n",
    "# cumu_24hour_return = ((rtn + 1).rolling(24).apply(np.prod, raw=True) - 1).dropna()\n",
    "# # print(cumu_24hour_return.info())\n",
    "\n",
    "\n",
    "# # 製作label和對應之features\n",
    "# y = np.array(rtn.Close[cumu_24hour_return.index[1]:cumu_24hour_return.index[-1]]).flatten()\n",
    "\n",
    "# x0 = np.array(rtn[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x1 = np.array(mean_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x2 = np.array(std_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x3 = np.array(median_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x4 = np.array(skew_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x5 = np.array(cumu_24hour_return[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# X = np.hstack([x0, x1, x2, x3, x4, x5])\n",
    "\n",
    "# print(x0.shape, x1.shape, x2.shape, x3.shape, x4.shape, x5.shape)\n",
    "# print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算指標及製作模型輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29466357308584684 0.34493426140757927 0.36040216550657383\n",
      "(2586, 4) (2586, 4) (2586, 4) (2586, 4)\n",
      "(2586,) (2586, 16)\n"
     ]
    }
   ],
   "source": [
    "data = df_AV\n",
    "# print(data.info())\n",
    "\n",
    "rtn = data.pct_change().dropna()\n",
    "# print(rtn.info())\n",
    "\n",
    "mean_last20day = rtn.rolling(20).mean().dropna()\n",
    "# print(mean_last20day.info())\n",
    "\n",
    "std_last20day = rtn.rolling(20).std().dropna()\n",
    "# print(std_last20day.info())\n",
    "\n",
    "median_last20day = rtn.rolling(20).median().dropna()\n",
    "# print(median_last20day.info())\n",
    "\n",
    "skew_last20day = 3 * (mean_last20day - median_last20day) / std_last20day\n",
    "# print(skew_last20day.info())\n",
    "\n",
    "cumu_20day_return = ((rtn + 1).rolling(20).apply(np.prod, raw=True) - 1).dropna()\n",
    "# print(cumu_20day_return.info())\n",
    "\n",
    "\n",
    "# 製作label和對應之features\n",
    "label = rtn.Close[cumu_20day_return.index[1]:cumu_20day_return.index[-1]]\n",
    "label[label.between(-.0015, .0015)] = 0\n",
    "label[label > 0] = 1\n",
    "label[label < 0] = 2\n",
    "print((label == 0).sum()/len(label), (label == 1).sum()/len(label), (label == 2).sum()/len(label))\n",
    "y = np.array(label).flatten()\n",
    "\n",
    "x0 = np.array(rtn[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "x1 = np.array(std_last20day[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "x2 = np.array(skew_last20day[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "x3 = np.array(cumu_20day_return[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "X = np.hstack([x0, x1, x2, x3])\n",
    "\n",
    "print(x0.shape, x1.shape, x2.shape, x3.shape)\n",
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 323014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集分割\n",
    "train_test_num = len(y)\n",
    "train_num = int(0.9 * train_test_num)\n",
    "X_train, X_test, y_train, y_test = X[:train_num], X[train_num:], y[:train_num], y[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_sample_acc: 99.83%\n",
      "in_sample_precision: [0.99709724 0.997558   1.        ]\n",
      "in_sample_recall: [1.         1.         0.99513973]\n",
      "in_sample_f1score: [0.99854651 0.99877751 0.99756395]\n",
      "\n",
      "out_sample_acc: 34.36%\n",
      "out_sample_precision: [0.35106383 0.23863636 0.45454545]\n",
      "out_sample_recall: [0.44       0.28       0.32110092]\n",
      "out_sample_f1score: [0.39053254 0.25766871 0.37634409]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 隨機森林\n",
    "\n",
    "# 透過Cross-Validation得到最佳超參數，並在整個訓練集上訓練\n",
    "parameters = {'n_estimators':list(range(10, 21, 10))}\n",
    "RFC = RandomForestClassifier(random_state=random_seed)\n",
    "RFCCV = GridSearchCV(RFC, parameters, cv=5, return_train_score=True)\n",
    "RFCCV.fit(X_train, y_train)\n",
    "\n",
    "# 衡量in_sample和out_sample表現\n",
    "in_sample_pred = RFCCV.predict(X_train)\n",
    "in_sample_acc = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "in_sample_precision_recall_f1score = precision_recall_fscore(y_train, in_sample_pred, labels=[0, 1, 2])\n",
    "out_sample_pred = RFCCV.predict(X_test)\n",
    "out_sample_acc = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "out_sample_precision_recall_f1score = precision_recall_fscore(y_test, out_sample_pred, labels=[0, 1, 2])\n",
    "\n",
    "# 配適結果：\n",
    "print(f'in_sample_acc: {in_sample_acc*100:.2f}%')\n",
    "print('in_sample_precision:', in_sample_precision_recall_f1score[0])\n",
    "print('in_sample_recall:', in_sample_precision_recall_f1score[1])\n",
    "print('in_sample_f1score:', in_sample_precision_recall_f1score[2])\n",
    "print()\n",
    "print(f'out_sample_acc: {out_sample_acc*100:.2f}%')\n",
    "print('out_sample_precision:', out_sample_precision_recall_f1score[0])\n",
    "print('out_sample_recall:', out_sample_precision_recall_f1score[1])\n",
    "print('out_sample_f1score:', out_sample_precision_recall_f1score[2])\n",
    "\n",
    "RFCCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Optim_hidden_size_with_earlystop(input_size, hidden_size_list, train_dataloader, valid_dataloader, epochs):\n",
    "    \n",
    "#     min_valid_loss_of_best_hidden_size = 100000\n",
    "#     min_valid_loss_hidden_size_with_epoch = (0, 0)\n",
    "#     for hidden_size in hidden_size_list:\n",
    "#         # model, criterion & optimizer\n",
    "#         model = NN(input_size, hidden_size)\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.Adam(model.parameters())\n",
    "        \n",
    "#         min_valid_loss_at_best_epoch = 100000\n",
    "#         min_valid_loss_epoch = 0\n",
    "#         for epoch in range(epochs):\n",
    "#             # training\n",
    "#             train_acc = 0\n",
    "#             len_train = 0\n",
    "#             model.train()\n",
    "#             for X, y in train_dataloader:\n",
    "#                 len_train += len(X)\n",
    "#                 # X, y = X.to(device), y.to(device)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 ypred = model(X)\n",
    "#                 # print('ypred: ', ypred, '\\n', 'y: ', y)\n",
    "#                 loss = criterion(ypred, y)\n",
    "#                 _, train_pred = torch.max(ypred, 1)\n",
    "#                 # print('ypred: ', ypred, '\\n', 'train_pred: ', train_pred)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#                 train_acc += (train_pred.cpu() == y.cpu()).sum().item()\n",
    "                \n",
    "#             # evaluating\n",
    "#             valid_loss = 0\n",
    "#             valid_acc = 0\n",
    "#             len_valid = 0\n",
    "#             model.eval()\n",
    "#             for X, y in  valid_dataloader:\n",
    "#                 len_valid += len(X)\n",
    "#                 # X, y = X.to(device), y.to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     ypred = model(X)\n",
    "#                     loss = criterion(ypred, y)\n",
    "#                     _, valid_pred = torch.max(ypred, 1)\n",
    "                \n",
    "#                     valid_acc += (valid_pred.cpu() == y.cpu()).sum().item()\n",
    "#                     valid_loss += loss.item() * len(X)\n",
    "#             valid_loss = valid_loss / len_valid\n",
    "#             # print(f'Train on hidden size: {hidden_size}, [{(epoch + 1):02d}/{epochs}]Epochs: Train_acc: {train_acc*100/len_train:.2f}% | Valid_acc: {valid_acc*100/len_valid:.2f}%')\n",
    "#             if valid_loss < min_valid_loss_at_best_epoch:\n",
    "#                 min_valid_loss_at_best_epoch = valid_loss\n",
    "#                 min_valid_loss_epoch = epoch + 1\n",
    "#         if min_valid_loss_at_best_epoch < min_valid_loss_of_best_hidden_size:\n",
    "#             min_valid_loss_of_best_hidden_size = min_valid_loss_at_best_epoch\n",
    "#             min_valid_loss_hidden_size_with_epoch = (hidden_size, min_valid_loss_epoch)\n",
    "    \n",
    "#     return min_valid_loss_hidden_size_with_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 找到最佳超參數hidden_size和對應之epochs(early stopping)\n",
    "# Optim_hidden_size, Optim_epoch = Optim_hidden_size_with_earlystop(input_size, hidden_size_list, train_dataloader, valid_dataloader, epochs)\n",
    "\n",
    "# # 以前述超參數訓練模型(在train_valid_set)\n",
    "# model = NN(input_size, Optim_hidden_size)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# for epoch in range(Optim_epoch):\n",
    "#     model.train()\n",
    "#     for X, y in train_valid_dataloader:\n",
    "#         # X, y = X.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         ypred = model(X)\n",
    "#         loss = criterion(ypred, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # 衡量in_sample和out_sample表現\n",
    "# model.eval()\n",
    "\n",
    "# in_sample_pred = np.array([])\n",
    "# for X, y in train_valid_dataloader:\n",
    "#     # X= X.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         ypred = model(X)\n",
    "#         _, ypred = torch.max(ypred, 1)\n",
    "#         in_sample_pred = np.append(in_sample_pred, ypred.cpu().numpy())\n",
    "\n",
    "# in_sample_acc = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "# in_sample_precision_recall_f1score = precision_recall_fscore(y_train, in_sample_pred, labels=[0, 1, 2])\n",
    "\n",
    "# out_sample_pred = np.array([])\n",
    "# for X, y in test_dataloader:\n",
    "#     # X= X.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         ypred = model(X)\n",
    "#         _, ypred = torch.max(ypred, 1)\n",
    "#         out_sample_pred = np.append(out_sample_pred, ypred.cpu().numpy())\n",
    "\n",
    "# out_sample_acc = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "# out_sample_precision_recall_f1score = precision_recall_fscore(y_test, out_sample_pred, labels=[0, 1, 2])\n",
    "\n",
    "# # 配適結果：\n",
    "# print(f'in_sample_acc: {in_sample_acc*100:.2f}%')\n",
    "# print('in_sample_precision:', in_sample_precision_recall_f1score[0])\n",
    "# print('in_sample_recall:', in_sample_precision_recall_f1score[1])\n",
    "# print('in_sample_f1score:', in_sample_precision_recall_f1score[2])\n",
    "# print()\n",
    "# print(f'out_sample_acc: {out_sample_acc*100:.2f}%')\n",
    "# print('out_sample_precision:', out_sample_precision_recall_f1score[0])\n",
    "# print('out_sample_recall:', out_sample_precision_recall_f1score[1])\n",
    "# print('out_sample_f1score:', out_sample_precision_recall_f1score[2])\n",
    "\n",
    "# Optim_hidden_size, Optim_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostumDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype('float32')\n",
    "        self.y = y.astype('long')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx])\n",
    "\n",
    "class model_structure(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.InputLayer = nn.Linear(input_size, hidden_size)\n",
    "        self.HiddenLayer =  nn.Sequential(nn.Linear(hidden_size, 2*hidden_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(2*hidden_size, 4*hidden_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(4*hidden_size, 2*hidden_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(2*hidden_size, hidden_size),\n",
    "                                          nn.ReLU()\n",
    "                                          )\n",
    "        self.OutputLayer = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.InputLayer(input)\n",
    "        hidden = self.HiddenLayer(hidden)\n",
    "        output = self.OutputLayer(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hparams: {'input_size': 16, 'hidden_size': 64, 'valid_ratio': 0.1, 'batch_size': 8, 'lr': 0.0001, 'num_epochs': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.5052531041069723\n",
      "valid_acc: 0.3261802575107296\n",
      "hparams: {'input_size': 16, 'hidden_size': 128, 'valid_ratio': 0.1, 'batch_size': 8, 'lr': 0.0001, 'num_epochs': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:38<00:13,  1.92it/s]"
     ]
    }
   ],
   "source": [
    "hparams = {'input_size': X_train.shape[1],\n",
    "           'hidden_size': 64,\n",
    "           'valid_ratio': 0.1,\n",
    "           'batch_size': 8,\n",
    "           'lr': 1e-4,\n",
    "           'num_epochs': 100}\n",
    "\n",
    "hidden_size_list = [64, 128, 256]\n",
    "\n",
    "# 隨機種子\n",
    "set_seed()\n",
    "\n",
    "# Dataset與DataLoader\n",
    "train_valid_set = CostumDataset(X_train, y_train)\n",
    "test_set = CostumDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader, valid_dataloader, train_valid_dataloader, test_dataloader = \\\n",
    "    setup_dataloader(hparams, train_valid_set, test_set)\n",
    "\n",
    "# 在 valid set 上最好的參數, 在 test set 上的準確度, 在 train_valid set 上的準確度, 超參數尋找過程中在 valid set 上最高的準確度\n",
    "best_hparams, best_model_valid_acc, best_model_train_acc, best_valid_acc = \\\n",
    "    Test(hidden_size_list, hparams, model_structure, train_dataloader, valid_dataloader, train_valid_dataloader, test_dataloader)\n",
    "\n",
    "print(best_hparams)\n",
    "print('best_model_valid_acc, best_model_train_acc, best_valid_acc')\n",
    "print(best_model_valid_acc, best_model_train_acc, best_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('quant_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26c4bd24ec080c28b188358f34a566e42734ce9a035f805760bf17c02bbd2a87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

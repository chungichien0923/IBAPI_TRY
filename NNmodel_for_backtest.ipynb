{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import IB_data, YF_data, AV_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import talib\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as precision_recall_fscore\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取資料(IB, yfinance or alpha_vantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_IB = IB_data('EUR', 'USD', endDateTime='20220726 23:59:59')\n",
    "# df_YF = YF_data('EUR', 'USD', start='2012-07-30', end='2022-07-26')\n",
    "df_AV = AV_data('EUR', 'USD', start='2012-07-30', end='2022-07-26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算指標(頻率為小時)及製作模型輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取出匯率每小時收盤價\n",
    "# data = df[['Close']]\n",
    "\n",
    "# # 計算匯率日報酬\n",
    "# day_data = data.resample('D').last().dropna()\n",
    "# day_rtn = day_data.pct_change().dropna()\n",
    "# day_rtn\n",
    "\n",
    "# hourly_std_byday = data.pct_change().resample('D').std().dropna()\n",
    "# hourly_skew_byday = 3 * (data.pct_change().resample('D').mean().dropna() - data.pct_change().resample('D').median().dropna()) \\\n",
    "#                        / data.pct_change().resample('D').std().dropna()\n",
    "# cumu_24day_return = ((day_rtn + 1).rolling(window=24).apply(np.prod, raw=True) - 1).dropna()\n",
    "\n",
    "# data = df\n",
    "# data = data.drop(index=data.index[-1])\n",
    "# # print(data.info())\n",
    "\n",
    "# rtn = data.pct_change().dropna()\n",
    "# # print(rtn.info())\n",
    "\n",
    "# mean_last24hour = rtn.rolling(24).mean().dropna()\n",
    "# # print(mean_last24hour.info())\n",
    "\n",
    "# std_last24hour = rtn.rolling(24).std().dropna()\n",
    "# # print(std_last24hour.info())\n",
    "\n",
    "# median_last24hour = rtn.rolling(24).median().dropna()\n",
    "# # print(median_last24hour.info())\n",
    "\n",
    "# skew_last24hour = 3 * (mean_last24hour - median_last24hour) / std_last24hour\n",
    "# # print(skew_last24hour.info())\n",
    "\n",
    "# cumu_24hour_return = ((rtn + 1).rolling(24).apply(np.prod, raw=True) - 1).dropna()\n",
    "# # print(cumu_24hour_return.info())\n",
    "\n",
    "\n",
    "# # 製作label和對應之features\n",
    "# y = np.array(rtn.Close[cumu_24hour_return.index[1]:cumu_24hour_return.index[-1]]).flatten()\n",
    "\n",
    "# x0 = np.array(rtn[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x1 = np.array(mean_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x2 = np.array(std_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x3 = np.array(median_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x4 = np.array(skew_last24hour[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# x5 = np.array(cumu_24hour_return[cumu_24hour_return.index[0]:cumu_24hour_return.index[-2]])\n",
    "# X = np.hstack([x0, x1, x2, x3, x4, x5])\n",
    "\n",
    "# print(x0.shape, x1.shape, x2.shape, x3.shape, x4.shape, x5.shape)\n",
    "# print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算指標及製作模型輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5297757153905646 0.2296983758700696 0.24052590873936583\n",
      "(2586, 4) (2586, 4) (2586, 4) (2586, 4)\n",
      "(2586,) (2586, 16)\n"
     ]
    }
   ],
   "source": [
    "data = df_AV\n",
    "# print(data.info())\n",
    "\n",
    "rtn = data.pct_change().dropna()\n",
    "# print(rtn.info())\n",
    "\n",
    "mean_last20day = rtn.rolling(20).mean().dropna()\n",
    "# print(mean_last20day.info())\n",
    "\n",
    "std_last20day = rtn.rolling(20).std().dropna()\n",
    "# print(std_last20day.info())\n",
    "\n",
    "median_last20day = rtn.rolling(20).median().dropna()\n",
    "# print(median_last20day.info())\n",
    "\n",
    "skew_last20day = 3 * (mean_last20day - median_last20day) / std_last20day\n",
    "# print(skew_last20day.info())\n",
    "\n",
    "cumu_20day_return = ((rtn + 1).rolling(20).apply(np.prod, raw=True) - 1).dropna()\n",
    "# print(cumu_20day_return.info())\n",
    "\n",
    "\n",
    "# 製作label和對應之features\n",
    "label = rtn.Close[cumu_20day_return.index[1]:cumu_20day_return.index[-1]]\n",
    "label[label.between(-.003, .003)] = 0\n",
    "label[label > 0] = 1\n",
    "label[label < 0] = 2\n",
    "print((label == 0).sum()/len(label), (label == 1).sum()/len(label), (label == 2).sum()/len(label))\n",
    "y = np.array(label).flatten()\n",
    "\n",
    "x0 = np.array(rtn[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "x1 = np.array(std_last20day[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "x2 = np.array(skew_last20day[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "x3 = np.array(cumu_20day_return[cumu_20day_return.index[0]:cumu_20day_return.index[-2]])\n",
    "X = np.hstack([x0, x1, x2, x3])\n",
    "\n",
    "print(x0.shape, x1.shape, x2.shape, x3.shape)\n",
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 323014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集分割\n",
    "train_test_num = len(y)\n",
    "train_num = int(0.9 * train_test_num)\n",
    "X_train, X_test, y_train, y_test = X[:train_num], X[train_num:], y[:train_num], y[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機森林\n",
    "\n",
    "# 透過Cross-Validation得到最佳超參數，並在整個訓練集上訓練\n",
    "parameters = {'n_estimators':list(range(10, 151, 10))}\n",
    "RFC = RandomForestClassifier(random_state=random_seed)\n",
    "RFCCV = GridSearchCV(RFC, parameters, cv=5, return_train_score=True)\n",
    "RFCCV.fit(X_train, y_train)\n",
    "\n",
    "# 衡量in_sample和out_sample表現\n",
    "in_sample_pred = RFCCV.predict(X_train)\n",
    "in_sample_acc = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "in_sample_precision_recall_f1score = precision_recall_fscore(y_train, in_sample_pred, labels=[0, 1, 2])\n",
    "out_sample_pred = RFCCV.predict(X_test)\n",
    "out_sample_acc = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "out_sample_precision_recall_f1score = precision_recall_fscore(y_test, out_sample_pred, labels=[0, 1, 2])\n",
    "\n",
    "# 配適結果：\n",
    "print(f'in_sample_acc: {in_sample_acc*100:.2f}%')\n",
    "print('in_sample_precision:', in_sample_precision_recall_f1score[0])\n",
    "print('in_sample_recall:', in_sample_precision_recall_f1score[1])\n",
    "print('in_sample_f1score:', in_sample_precision_recall_f1score[2])\n",
    "print()\n",
    "print(f'out_sample_acc: {out_sample_acc*100:.2f}%')\n",
    "print('out_sample_precision:', out_sample_precision_recall_f1score[0])\n",
    "print('out_sample_recall:', out_sample_precision_recall_f1score[1])\n",
    "print('out_sample_f1score:', out_sample_precision_recall_f1score[2])\n",
    "\n",
    "RFCCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神經網路所需class和function\n",
    "\n",
    "class CostumDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype('float32')\n",
    "        self.y = y.astype('long')#np.expand_dims(y, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx])\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.InputLayer = nn.Linear(input_size, hidden_size)\n",
    "        self.HiddenLayer =  nn.Sequential(nn.Linear(hidden_size, 2*hidden_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(2*hidden_size, 4*hidden_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(4*hidden_size, 2*hidden_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(2*hidden_size, hidden_size),\n",
    "                                          nn.ReLU()\n",
    "                                          )\n",
    "        self.OutputLayer = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.InputLayer(input)\n",
    "        hidden = self.HiddenLayer(hidden)\n",
    "        output = self.OutputLayer(hidden)\n",
    "        return output\n",
    "\n",
    "def Optim_hidden_size_with_earlystop(input_size, hidden_size_list, train_dataloader, valid_dataloader, epochs):\n",
    "    \n",
    "    min_valid_loss_of_best_hidden_size = 100000\n",
    "    min_valid_loss_hidden_size_with_epoch = (0, 0)\n",
    "    for hidden_size in hidden_size_list:\n",
    "        # model, criterion & optimizer\n",
    "        model = NN(input_size, hidden_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        \n",
    "        min_valid_loss_at_best_epoch = 100000\n",
    "        min_valid_loss_epoch = 0\n",
    "        for epoch in range(epochs):\n",
    "            # training\n",
    "            train_acc = 0\n",
    "            len_train = 0\n",
    "            model.train()\n",
    "            for X, y in train_dataloader:\n",
    "                len_train += len(X)\n",
    "                # X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                ypred = model(X)\n",
    "                # print('ypred: ', ypred, '\\n', 'y: ', y)\n",
    "                loss = criterion(ypred, y)\n",
    "                _, train_pred = torch.max(ypred, 1)\n",
    "                # print('ypred: ', ypred, '\\n', 'train_pred: ', train_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_acc += (train_pred.cpu() == y.cpu()).sum().item()\n",
    "                \n",
    "            # evaluating\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            len_valid = 0\n",
    "            model.eval()\n",
    "            for X, y in  valid_dataloader:\n",
    "                len_valid += len(X)\n",
    "                # X, y = X.to(device), y.to(device)\n",
    "                with torch.no_grad():\n",
    "                    ypred = model(X)\n",
    "                    loss = criterion(ypred, y)\n",
    "                    _, valid_pred = torch.max(ypred, 1)\n",
    "                \n",
    "                    valid_acc += (valid_pred.cpu() == y.cpu()).sum().item()\n",
    "                    valid_loss += loss.item() * len(X)\n",
    "            valid_loss = valid_loss / len_valid\n",
    "            # print(f'Train on hidden size: {hidden_size}, [{(epoch + 1):02d}/{epochs}]Epochs: Train_acc: {train_acc*100/len_train:.2f}% | Valid_acc: {valid_acc*100/len_valid:.2f}%')\n",
    "            if valid_loss < min_valid_loss_at_best_epoch:\n",
    "                min_valid_loss_at_best_epoch = valid_loss\n",
    "                min_valid_loss_epoch = epoch + 1\n",
    "        if min_valid_loss_at_best_epoch < min_valid_loss_of_best_hidden_size:\n",
    "            min_valid_loss_of_best_hidden_size = min_valid_loss_at_best_epoch\n",
    "            min_valid_loss_hidden_size_with_epoch = (hidden_size, min_valid_loss_epoch)\n",
    "    \n",
    "    return min_valid_loss_hidden_size_with_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神經網路\n",
    "\n",
    "# 輸入大小\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# 超參數\n",
    "epochs = 150\n",
    "batch_size = 8\n",
    "hidden_size_list = [64, 128, 256]\n",
    "valid_ratio = 0.1\n",
    "\n",
    "# 隨機種子\n",
    "# Python random module\n",
    "random.seed(random_seed)\n",
    "# Numpy\n",
    "np.random.seed(random_seed)\n",
    "# Torch\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Dataset與DataLoader\n",
    "train_valid_set = CostumDataset(X_train, y_train)\n",
    "test_set = CostumDataset(X_test, y_test)\n",
    "\n",
    "train_valid_num = len(train_valid_set)\n",
    "train_num = int((1 - valid_ratio) * train_valid_num)\n",
    "valid_num = train_valid_num - train_num\n",
    "\n",
    "train_set, valid_set = random_split(train_valid_set, [train_num, valid_num])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=batch_size)\n",
    "train_valid_dataloader = DataLoader(train_valid_set)\n",
    "test_dataloader = DataLoader(test_set)\n",
    "\n",
    "# 找到最佳超參數hidden_size和對應之epochs(early stopping)\n",
    "Optim_hidden_size, Optim_epoch = Optim_hidden_size_with_earlystop(input_size, hidden_size_list, train_dataloader, valid_dataloader, epochs)\n",
    "\n",
    "# 以前述超參數訓練模型(在train_valid_set)\n",
    "model = NN(input_size, Optim_hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(Optim_epoch):\n",
    "    model.train()\n",
    "    for X, y in train_valid_dataloader:\n",
    "        # X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(X)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 衡量in_sample和out_sample表現\n",
    "model.eval()\n",
    "\n",
    "in_sample_pred = np.array([])\n",
    "for X, y in train_valid_dataloader:\n",
    "    # X= X.to(device)\n",
    "    with torch.no_grad():\n",
    "        ypred = model(X)\n",
    "        _, ypred = torch.max(ypred, 1)\n",
    "        in_sample_pred = np.append(in_sample_pred, ypred.cpu().numpy())\n",
    "\n",
    "in_sample_acc = (in_sample_pred == y_train).sum()/len(y_train)\n",
    "in_sample_precision_recall_f1score = precision_recall_fscore(y_train, in_sample_pred, labels=[0, 1, 2])\n",
    "\n",
    "out_sample_pred = np.array([])\n",
    "for X, y in test_dataloader:\n",
    "    # X= X.to(device)\n",
    "    with torch.no_grad():\n",
    "        ypred = model(X)\n",
    "        _, ypred = torch.max(ypred, 1)\n",
    "        out_sample_pred = np.append(out_sample_pred, ypred.cpu().numpy())\n",
    "\n",
    "out_sample_acc = (out_sample_pred == y_test).sum()/len(y_test)\n",
    "out_sample_precision_recall_f1score = precision_recall_fscore(y_test, out_sample_pred, labels=[0, 1, 2])\n",
    "\n",
    "# 配適結果：\n",
    "print(f'in_sample_acc: {in_sample_acc*100:.2f}%')\n",
    "print('in_sample_precision:', in_sample_precision_recall_f1score[0])\n",
    "print('in_sample_recall:', in_sample_precision_recall_f1score[1])\n",
    "print('in_sample_f1score:', in_sample_precision_recall_f1score[2])\n",
    "print()\n",
    "print(f'out_sample_acc: {out_sample_acc*100:.2f}%')\n",
    "print('out_sample_precision:', out_sample_precision_recall_f1score[0])\n",
    "print('out_sample_recall:', out_sample_precision_recall_f1score[1])\n",
    "print('out_sample_f1score:', out_sample_precision_recall_f1score[2])\n",
    "\n",
    "Optim_hidden_size, Optim_epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('quant_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26c4bd24ec080c28b188358f34a566e42734ce9a035f805760bf17c02bbd2a87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
